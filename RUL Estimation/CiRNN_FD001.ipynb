{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8b105c",
      "metadata": {
        "id": "ab8b105c",
        "outputId": "ce9fe923-a050-4999-825f-692df5b070f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rashmi/PythonProjects/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f81e211a550>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "seed = 40\n",
        "os.environ['PYTHONHASHSEED']=str(seed)\n",
        "\n",
        "import random\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib.pyplot import *\n",
        "style.use('ggplot')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.profiler\n",
        "import torch.autograd.profiler as profiler\n",
        "import sklearn.preprocessing as preprocess\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "import optuna\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c6be63e",
      "metadata": {
        "id": "9c6be63e"
      },
      "source": [
        "# Context Integrated RNN - CiRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85c75a2",
      "metadata": {
        "id": "e85c75a2",
        "outputId": "2a90c9d7-c75b-4dbf-8ce7-cecc7eab44ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get CPU or GPU device for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class ContextGRU(torch.nn.Module):\n",
        "    \"\"\"\n",
        "     simple GRU cell network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, context_dim):\n",
        "        super(ContextGRU, self).__init__()\n",
        "\n",
        "        self.n_x = input_dim\n",
        "        self.n_h = hidden_dim\n",
        "        self.n_y = output_dim\n",
        "        self.n_z = context_dim\n",
        "        #self.m = 9  #dimension of basis function vector (polynomial features) for 3 context features\n",
        "        self.m = 5  #dimension of basis function vector (polynomial features) for 2 context features\n",
        "\n",
        "\n",
        "        # reset gate components\n",
        "        self.linear_reset_w1 = nn.Linear(self.n_x * self.m, self.n_h, bias=True)\n",
        "        self.linear_reset_r1 = nn.Linear(self.n_h, self.n_h, bias=True)\n",
        "\n",
        "\n",
        "        self.linear_reset_w2 = nn.Linear(self.n_x * self.m, self.n_h, bias=True)\n",
        "        self.linear_reset_r2 = nn.Linear(self.n_h, self.n_h, bias=True)\n",
        "        self.activation_1 = nn.Sigmoid()\n",
        "\n",
        "        # update gate components\n",
        "        self.linear_gate_w3 = nn.Linear(self.n_x * self.m, self.n_h, bias=True)\n",
        "        self.linear_gate_r3 = nn.Linear(self.n_h, self.n_h, bias=True)\n",
        "        self.activation_2 = nn.Sigmoid()\n",
        "\n",
        "        self.activation_3 = nn.Tanh()\n",
        "\n",
        "        #output\n",
        "        self.linear_output = nn.Linear(self.n_h, self.n_y, bias=True)\n",
        "\n",
        "\n",
        "    def reset_gate(self, xg, h):  #xg is the kronecker product of x and  basis function G(z)\n",
        "        x_1 = self.linear_reset_w1(xg)\n",
        "        h_1 = self.linear_reset_r1(h)\n",
        "        # gate update\n",
        "        r = self.activation_1(x_1 + h_1)\n",
        "        return r\n",
        "\n",
        "    def update_gate(self, xg, h):\n",
        "        x_2 = self.linear_reset_w2(xg)\n",
        "        h_2 = self.linear_reset_r2(h)\n",
        "        s = self.activation_2( h_2 + x_2)\n",
        "        return s\n",
        "\n",
        "\n",
        "    def update_component(self, xg, h, r):\n",
        "        x_3 = self.linear_gate_w3(xg)\n",
        "        h_3 = r * self.linear_gate_r3(h)\n",
        "        h_tilda = self.activation_3(x_3+h_3)\n",
        "        return h_tilda\n",
        "\n",
        "\n",
        "    def compute_output(self,h):\n",
        "        y_pred = self.linear_output(h)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def cell_forward(self, x, h, G):\n",
        "\n",
        "        \"\"\"\n",
        "        Implements a single forward step of the Context GRU-cell\n",
        "\n",
        "        Input Arguments:\n",
        "            x (mini-batch): input x at time step t , (n,n_x) : (batch_size, input_dim)\n",
        "            h : hidden state at time step t-1, (n,n_h) : (batch_size, hidden_dim)\n",
        "            G : vector of basis funcitons (m,n)\n",
        "\n",
        "        Returns:\n",
        "            h_new: hidden state at time step t, (n,n_h)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # kronecker product of x and G(zt)\n",
        "        n = x.shape[0]\n",
        "        xg = torch.zeros(n,self.n_x*self.m).to(device)\n",
        "\n",
        "        for i in range(n):\n",
        "\n",
        "            xg[i,:] = torch.kron(x[i,:],G[:,i])\n",
        "\n",
        "\n",
        "        # Equation 1. reset gate vector\n",
        "        r = self.reset_gate(xg, h)\n",
        "\n",
        "        # Equation 2: the update gate - the shared update gate vector z\n",
        "        s = self.update_gate(xg, h)\n",
        "\n",
        "        # Equation 3: The almost output component\n",
        "        h_tilda = self.update_component(xg,h,r)\n",
        "\n",
        "        # Equation 4: the new hidden state\n",
        "        h_new = (1-s) * h_tilda  + s * h\n",
        "\n",
        "        #output\n",
        "\n",
        "        y_pred = self.compute_output(h)\n",
        "\n",
        "        return h_new, y_pred\n",
        "\n",
        "\n",
        "    def forward(self, x, z):\n",
        "\n",
        "        \"\"\"\n",
        "        Implement the forward propagation of the recurrent neural network\n",
        "\n",
        "        Input Arguments:\n",
        "        x (mini_batch): primary input for every time-step in mini-batches of shape (n, T, n_x)\n",
        "        z (mini_batch): context input for every time-step in mini-batches of shape (n,T,n_z)\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            h -- Hidden states for every time-step, numpy array of shape (n, T, n_h)\n",
        "            y_pred -- Predictions for every time-step, numpy array of shape (n, T, n_y),\n",
        "            here T is 1 for Seq to Vec RNN\n",
        "        \"\"\"\n",
        "\n",
        "        # Retrieve dimensions from shapes of x\n",
        "        #print(x.shape)\n",
        "        #print(z.shape)\n",
        "        n,T,n_x = x.shape\n",
        "        n_y = self.n_y\n",
        "        n_h = self.n_h\n",
        "        n_z = self.n_z\n",
        "\n",
        "\n",
        "\n",
        "        # initialize \"h\"\n",
        "\n",
        "        h = self.init_hidden(n,T,n_h)\n",
        "\n",
        "        #y_pred = np.zeros((m,T_x,n_y))\n",
        "        #y_pred is single value for one sample, m=1\n",
        "\n",
        "        #basis function vector\n",
        "        G = self.apply_basis(z[:,0,:])  #G: size of (n,m)\n",
        "\n",
        "        #for initial time step the hidden state is 0\n",
        "        h_temp = h.clone()\n",
        "        h_init = h_temp[:,0,:]\n",
        "        h_curr, y_curr = self.cell_forward(x[:,0,:],h_init,torch.t(G))\n",
        "\n",
        "        # loop over all time-steps\n",
        "        for t in range(1,T):\n",
        "\n",
        "            #compute the vector of basis functions\n",
        "\n",
        "            G = self.apply_basis(z[:,t,:])  #G: size of (n,m)\n",
        "\n",
        "            # Update next hidden state\n",
        "            # ignore yt_pred for seq to vector\n",
        "            h[:,t,:]= h_curr\n",
        "            h_temp = h.clone()\n",
        "            h_prev = h_temp[:,t,:]  #h_prev: (n,n_h)\n",
        "            h_curr, y_curr = self.cell_forward(x[:,t,:],h_prev, torch.t(G))\n",
        "\n",
        "            #y_pred[t,:] = yt_pred\n",
        "\n",
        "\n",
        "        #compute the predicted output from the last cell i.e at last time step T\n",
        "        y_pred = torch.zeros(n,1,1,device = 'cuda:0')\n",
        "\n",
        "        #get the value of y_pred from the last cell\n",
        "        y_pred[:,0,:] = y_curr\n",
        "\n",
        "        #print(y_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "        return h, y_pred\n",
        "\n",
        "\n",
        "    def init_hidden(self, n:int,T:int, n_h:int):\n",
        "        #initialise the hidden state\n",
        "        #n : batch-size\n",
        "        #T : Input sequence length\n",
        "        #returns h of size (n,T,n_h)\n",
        "        return torch.zeros(n,T,n_h,device = 'cuda:0')\n",
        "\n",
        "\n",
        "    def apply_basis(self,zt):\n",
        "        '''\n",
        "        apply the basis function: polynomial degree 2\n",
        "        [z0, z1, z2, z0z0, z0z1, z0z2....]\n",
        "        input arguments:\n",
        "            zt: context vector (n,n_z) for mini-batch of size n and n_z context dim\n",
        "        Returns:\n",
        "            G : tensor of basis functions, (m,n)\n",
        "\n",
        "\n",
        "        '''\n",
        "\n",
        "        #poly = PolynomialFeatures(2, include_bias=False, interaction_only=True)\n",
        "        poly = PolynomialFeatures(2, include_bias=False)\n",
        "        G = torch.tensor(poly.fit_transform(zt.cpu().numpy())).to(device) #fit_transform returns nd array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return G\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999d31f3",
      "metadata": {
        "id": "999d31f3"
      },
      "outputs": [],
      "source": [
        "class Optimization:\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train_step(self, x, y, z):\n",
        "\n",
        "       # with profiler.record_function(\"TRAIN STEP FUNCTION\"):\n",
        "        # Sets model to train mode\n",
        "        self.model.train()\n",
        "\n",
        "        # Makes predictions\n",
        "        h, yhat = self.model(x, z)\n",
        "\n",
        "\n",
        "        # Computes loss\n",
        "        loss = self.loss_fn(y, yhat)\n",
        "\n",
        "        #with profiler.record_function(\"LOSS_BACKWARD\"):\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Updates parameters and zeroes gradients\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Returns the loss\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, train_loader, val_loader, batch_size, n_epochs=50, np_features=1, nc_features=1):\n",
        "        '''\n",
        "        np_features = # primary input features\n",
        "        nc_features = # context input features\n",
        "        '''\n",
        "        #model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
        "        times = []\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "            start_epoch = time.time()\n",
        "\n",
        "            batch_losses = []\n",
        "            for x_batch, z_batch, y_batch in train_loader:\n",
        "                x_batch = x_batch.view([batch_size,-1, np_features]).to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                z_batch = z_batch.view([batch_size,-1, nc_features]).to(device)\n",
        "\n",
        "                #with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "                loss = self.train_step(x_batch, y_batch, z_batch)\n",
        "                #print(prof.key_averages(group_by_stack_n=5).table(sort_by = 'self_cpu_time_total', row_limit = 5))\n",
        "\n",
        "                batch_losses.append(loss)\n",
        "\n",
        "\n",
        "            training_loss = np.mean(batch_losses)\n",
        "            self.train_losses.append(training_loss)\n",
        "\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_val_losses = []\n",
        "                for x_val, z_val, y_val in val_loader:\n",
        "                    x_val = x_val.view([batch_size, -1, np_features]).to(device, non_blocking=True)\n",
        "                    y_val = y_val.to(device)\n",
        "                    z_val = z_val.view([batch_size, -1, nc_features]).to(device,non_blocking=True)\n",
        "                    self.model.eval()\n",
        "\n",
        "                    # with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "                    h,yhat = self.model(x_val, z_val)\n",
        "                    # print(prof.key_averages(group_by_stack_n=5).table(sort_by = 'self_cpu_time_total', row_limit = 5))\n",
        "\n",
        "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
        "                    batch_val_losses.append(val_loss)\n",
        "                validation_loss = np.mean(batch_val_losses)\n",
        "                self.val_losses.append(validation_loss)\n",
        "\n",
        "            if (epoch % 5 == 0):\n",
        "                print(\n",
        "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
        "                )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            end_epoch = time.time()\n",
        "            elapsed = end_epoch - start_epoch\n",
        "            times.append(elapsed)\n",
        "\n",
        "        total_time = sum(times)\n",
        "        avg_time = sum(times)/n_epochs\n",
        "\n",
        "        print(f\"Average Training time: {avg_time:.4f} s for epochs {n_epochs}\")\n",
        "\n",
        "        print(f\"Total Training time: {total_time:.4f} s for epochs {n_epochs}\")\n",
        "\n",
        "\n",
        "        #torch.save(self.model.state_dict(), model_path)\n",
        "\n",
        "        return validation_loss  #this will be used by otuna to optimize\n",
        "\n",
        "    def evaluate(self, test_loader, batch_size=1, np_features=1, nc_features = 1):\n",
        "            with torch.no_grad():\n",
        "                predictions = []\n",
        "                values = []\n",
        "                for x_test, z_test, y_test in test_loader:\n",
        "\n",
        "                    x_test = x_test.view([batch_size,-1, np_features]).to(device, non_blocking=True)\n",
        "                    y_test = y_test.to(device)\n",
        "                    z_test = z_test.view([batch_size,-1, nc_features]).to(device, non_blocking=True)\n",
        "                    self.model.eval()\n",
        "                    h,yhat = self.model(x_test, z_test)\n",
        "                    predictions.append(yhat.detach().cpu().numpy())\n",
        "                    values.append(y_test.detach().cpu().numpy())\n",
        "\n",
        "            return predictions, values\n",
        "\n",
        "    def plot_losses(self):\n",
        "            plt.plot(self.train_losses, label=\"Training loss\")\n",
        "            plt.plot(self.val_losses, label=\"Validation loss\")\n",
        "            plt.legend()\n",
        "            plt.title(\"Losses\")\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.ylabel(\"Loss(MSE)\")\n",
        "            plt.show()\n",
        "            plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0211805",
      "metadata": {
        "id": "f0211805"
      },
      "source": [
        "## 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3febeddf",
      "metadata": {
        "id": "3febeddf"
      },
      "outputs": [],
      "source": [
        "#data load function\n",
        "#4 sets of data: FD001, FD002, FD003, FD004\n",
        "#FD001 and FD003 same operating condition 1\n",
        "#FD002 and FD004 same operating condition 6\n",
        "\n",
        "def dataload(filename):\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca73a50",
      "metadata": {
        "id": "4ca73a50"
      },
      "outputs": [],
      "source": [
        "#define path to the data folder\n",
        "#path ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c0d28aa6",
      "metadata": {
        "id": "c0d28aa6"
      },
      "outputs": [],
      "source": [
        "train_FD001 = dataload(path+'train_FD001')\n",
        "test_FD001 = dataload(path+'test_FD001')\n",
        "#drop the frst unnamed column\n",
        "train_FD001.drop(columns = train_FD001.columns[0],axis=1, inplace=True)\n",
        "test_FD001.drop(columns = test_FD001.columns[0],axis=1, inplace=True)\n",
        "train_FD001.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b1b404",
      "metadata": {
        "id": "69b1b404"
      },
      "source": [
        "## 2. Data Prepreprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e38b43",
      "metadata": {
        "id": "e6e38b43"
      },
      "outputs": [],
      "source": [
        "# selected features FD001 after data analysis\n",
        "\n",
        "feature_list1 = ['unit_number','time_cycles','setting_1', 'setting_2',\n",
        "                's_2','s_3','s_4','s_7','s_8','s_9','s_11','s_12','s_13',\n",
        "                 's_15','s_17','s_20','s_21','RUL']   #feature list for FD001\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb158818",
      "metadata": {
        "id": "fb158818"
      },
      "source": [
        "### a) Smoothing - moving average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf6727f",
      "metadata": {
        "id": "caf6727f"
      },
      "outputs": [],
      "source": [
        "#Trailing moving average\n",
        "#trail_ma(t) = mean(obs(t-2), obs(t-1), obs(t))\n",
        "\n",
        "def moving_average(x, w):\n",
        "    #x: time series\n",
        "    #w: sliding window size\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3d8406",
      "metadata": {
        "id": "1a3d8406"
      },
      "source": [
        "### b) Normalisation - Transform and Inverse transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b80d6598",
      "metadata": {
        "id": "b80d6598"
      },
      "outputs": [],
      "source": [
        "def data_transform(data,option ='std'):\n",
        "#data is numpy array\n",
        "#option is set to std for standardization or minmax\n",
        "\n",
        "    n = data.shape[0]\n",
        "\n",
        "    if option == 'std' :\n",
        "\n",
        "        #perform standardization of data\n",
        "        miu = np.mean(data,axis = 0)\n",
        "        sigma = np.std(data,axis=0,dtype=float)\n",
        "        temp_data = data-np.tile(miu,(n,1))\n",
        "        std_data = np.divide(temp_data,np.tile(sigma,(n,1)))\n",
        "\n",
        "        return std_data, miu, sigma\n",
        "\n",
        "    elif option == 'minmax':\n",
        "\n",
        "        #perform min-max normalization\n",
        "        max_val = np.max(data,0)\n",
        "        #print(max_val)\n",
        "        min_val = np.min(data,0)\n",
        "        #print(min_val)\n",
        "        rng = max_val-min_val\n",
        "        norm_data = np.divide(data - np.tile(min_val,(n,1)),np.tile(rng,(n,1)))\n",
        "\n",
        "        return norm_data, min_val, rng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f929d8",
      "metadata": {
        "id": "c3f929d8"
      },
      "outputs": [],
      "source": [
        "## Inverse transform the target/outout data from normalized to original\n",
        "\n",
        "def inv_trans(data,option,param1, param2):\n",
        "    # apply inverse of standardization or normalization for 1-D column/row vector\n",
        "    # option: standard or minmax normalization\n",
        "    # params:list of parameters applied while normalization\n",
        "    # data is 1-D column vector\n",
        "#     print(data)\n",
        "#     print(param1)\n",
        "#     print(param2)\n",
        "\n",
        "    if option == \"std\":\n",
        "         #perform standardization of data\n",
        "        miu = param1\n",
        "        sigma = param2\n",
        "        inv_data = data*sigma + miu\n",
        "\n",
        "        return inv_data\n",
        "\n",
        "    else : #MinMax normalization\n",
        "\n",
        "        #perform min-max normalization\n",
        "        min_val = param1\n",
        "        rng = param2\n",
        "        inv_data = data*rng+min_val\n",
        "        return inv_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b29c4780",
      "metadata": {
        "id": "b29c4780"
      },
      "source": [
        "### c) Denormalization (target) - 1 level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81216c35",
      "metadata": {
        "id": "81216c35"
      },
      "outputs": [],
      "source": [
        "# Inverse transform the target/outout data from normalized to original\n",
        "\n",
        "def d_norm_1(data,option,param1, param2):\n",
        "    # apply inverse of standardization or normalization for 1-D column/row vector\n",
        "    # option: standard or minmax normalization\n",
        "    # param:parameters applied while normalization (training data statistics)\n",
        "    # data is 1-D column vector\n",
        "\n",
        "\n",
        "\n",
        "    if option == \"std\":  #z-score denormalization\n",
        "\n",
        "\n",
        "        miu = param1\n",
        "        sigma = param2\n",
        "        inv_data1 = data*sigma+miu\n",
        "\n",
        "\n",
        "    else : #MinMax denormalization\n",
        "\n",
        "            m = param1\n",
        "            r = param2\n",
        "            inv_data1 = data*r+m\n",
        "\n",
        "    #print (inv_data1.to_numpy())\n",
        "\n",
        "    return inv_data1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79de2e33",
      "metadata": {
        "id": "79de2e33"
      },
      "source": [
        "### d) Minmax data normalisation and smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd8f7fa",
      "metadata": {
        "id": "0fd8f7fa",
        "outputId": "302ef7fa-c666-4669-8afa-12bf145191d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20431, 18)\n"
          ]
        }
      ],
      "source": [
        "#Train data normalisation and smoothing\n",
        "\n",
        "select_data_train = np.array(train_FD001[feature_list1].copy(deep=True))\n",
        "train_data, p1, p2 = data_transform(select_data_train[:,2:],option = 'minmax') #drop unit_number and time_cycles\n",
        "\n",
        "\n",
        "#add unit_number and time_cycles to train_data\n",
        "train_data = np.concatenate((select_data_train[:,0:2], train_data), axis = 1)\n",
        "\n",
        "w = 3   #window size for moving average smoothing\n",
        "\n",
        "#Apply data smoothing separately to each engine unit data group\n",
        "unit_numbers = train_FD001['unit_number'].unique()\n",
        "#unit_numbers = train_FD004['unit_number'].unique()\n",
        "\n",
        "l = len(feature_list1)\n",
        "\n",
        "#for unit number 1\n",
        "grp_data = train_data[train_data[:,0] == 1]\n",
        "(n,m) = grp_data.shape\n",
        "smooth_data_train = np.zeros((n-w+1,m))  #array to store smooth data\n",
        "smooth_data_train[:,0:2] = grp_data[(w-1):,0:2]  #copy unit number and  time_cycle\n",
        "smooth_data_train[:,l-1] = grp_data[(w-1):,l-1] #copy RUL\n",
        "#print(smooth_data[0:4,:])\n",
        "\n",
        "for i in range(l-3): #number of features - unit_number, time_cycles and RUL\n",
        "    series = grp_data[:,i+2]\n",
        "    smooth_series = moving_average(series,w)\n",
        "    smooth_data_train[:,i+2] = smooth_series\n",
        "\n",
        "unit_numbers = unit_numbers[1:]\n",
        "#for remaining unit numbers\n",
        "for i in unit_numbers:\n",
        "    grp_data = train_data[train_data[:,0] == i]\n",
        "    (n,m) = grp_data.shape\n",
        "    smooth_data_train1= np.zeros((n-w+1,m))  #array to store smooth data\n",
        "    smooth_data_train1[:,0:2] = grp_data[(w-1):,0:2]  #copy unit number, time_cycle, and 3 settings\n",
        "    smooth_data_train1[:,l-1] = grp_data[(w-1):,l-1] #copy RUL\n",
        "    #print(smooth_data[0:4,:])\n",
        "\n",
        "    for i in range(l-3): #4 sensors+3 settings\n",
        "        series = grp_data[:,i+2]\n",
        "        smooth_series = moving_average(series,w)\n",
        "        smooth_data_train1[:,i+2] = smooth_series\n",
        "\n",
        "    smooth_data_train = np.concatenate((smooth_data_train,smooth_data_train1), axis = 0)\n",
        "\n",
        "\n",
        "print(smooth_data_train.shape)\n",
        "\n",
        "#plot smooth data for selected engine unit\n",
        "\n",
        "# data = smooth_data_train[smooth_data_train[:,0] == 100]\n",
        "# #print(data[0:5,:])\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(2,11):\n",
        "#     plt.subplot(3, 3, i-1).set_title(feature_list[i])\n",
        "#     plt.plot(data[:,1], data[:,i])\n",
        "#     plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3dc1757",
      "metadata": {
        "id": "c3dc1757",
        "outputId": "12f700d6-eb00-48c4-cfe6-f8de725e1d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12896, 18)\n"
          ]
        }
      ],
      "source": [
        "#Test data normalisation and smoothing\n",
        "\n",
        "#1. Normalize using training data statistics\n",
        "select_data_test = np.array(test_FD001[feature_list1].copy(deep=True))\n",
        "\n",
        "#perform min-max normalization\n",
        "min_val = p1\n",
        "rng = p2\n",
        "test_data = (select_data_test[:,2:]-min_val)/rng\n",
        "\n",
        "#add the unit_number and time_cycles\n",
        "test_data = np.concatenate((select_data_test[:,0:2], test_data), axis = 1)\n",
        "\n",
        "w = 3   #window size for moving average smoothing\n",
        "\n",
        "#Apply data smoothing separately to each engine unit data group\n",
        "unit_numbers = test_FD001['unit_number'].unique()\n",
        "\n",
        "l = len(feature_list1)\n",
        "\n",
        "#for unit number 1\n",
        "grp_data = test_data[test_data[:,0] == 1]\n",
        "(n,m) = grp_data.shape\n",
        "smooth_data_test = np.zeros((n-w+1,m))  #array to store smooth data\n",
        "smooth_data_test[:,0:2] = grp_data[(w-1):,0:2]  #copy unit number and  time_cycle\n",
        "smooth_data_test[:,l-1] = grp_data[(w-1):,l-1] #copy RUL\n",
        "\n",
        "for i in range(l-3): #4 sensors + 3 settings\n",
        "    series = grp_data[:,i+2]\n",
        "    smooth_series = moving_average(series,w)\n",
        "    smooth_data_test[:,i+2] = smooth_series\n",
        "\n",
        "unit_numbers = unit_numbers[1:]\n",
        "#for remaining unit numbers\n",
        "for i in unit_numbers:\n",
        "\n",
        "    grp_data = test_data[test_data[:,0] == i]\n",
        "    (n,m) = grp_data.shape\n",
        "\n",
        "    smooth_data_test1= np.zeros((n-w+1,m))  #array to store smooth data\n",
        "    smooth_data_test1[:,0:2] = grp_data[(w-1):,0:2]  #copy unit number, time_cycle, and 3 settings\n",
        "    smooth_data_test1[:,l-1] = grp_data[(w-1):,l-1] #copy RUL\n",
        "\n",
        "    for i in range(l-3): #4 sensors+3 settings\n",
        "        series = grp_data[:,i+2]\n",
        "        smooth_series = moving_average(series,w)\n",
        "        smooth_data_test1[:,i+2] = smooth_series\n",
        "\n",
        "    smooth_data_test = np.concatenate((smooth_data_test,smooth_data_test1), axis = 0)\n",
        "\n",
        "\n",
        "print(smooth_data_test.shape)\n",
        "\n",
        "#plot smooth data for selected engine unit\n",
        "\n",
        "# data = smooth_data_test[smooth_data_test[:,0] == 100]\n",
        "# #print(data[0:5,:])\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(2,11):\n",
        "#     plt.subplot(3, 3, i-1).set_title(feature_list[i])\n",
        "#     plt.plot(data[:,1], data[:,i])\n",
        "#     plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1465b2c",
      "metadata": {
        "id": "c1465b2c"
      },
      "source": [
        "## 3. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f2a335",
      "metadata": {
        "id": "02f2a335"
      },
      "outputs": [],
      "source": [
        "# Prepare the data for RNN model such that the data is presented as (num_samples,seq_length,num_features)\n",
        "\n",
        "def data_preparation(data,n_past,n_future):\n",
        "    '''\n",
        "    input:\n",
        "        data :[unit_number, time_cycles,context inputs, primary inputs, output]\n",
        "        n_past : number of past steps to be used for prediction\n",
        "        n_future :  number of steps ahead\n",
        "\n",
        "    returns:\n",
        "        context input (Z): 'setting_1','setting_2', 'setting_3'\n",
        "        primary input (X): 's_2','s_8','s_14','s_16'\n",
        "        ouput/target (Y): 'RUL' at time step t\n",
        "        Engine unit and time cycles (U)\n",
        "    '''\n",
        "\n",
        "    n,m = data.shape\n",
        "    #print((n,m))\n",
        "\n",
        "\n",
        "    k = n_future\n",
        "    t = n_past\n",
        "\n",
        "\n",
        "    input_data = []\n",
        "    output_data = []\n",
        "    context_data = []\n",
        "    engine_data = []\n",
        "\n",
        "\n",
        "    for i in range(t, (n-k+1)):\n",
        "\n",
        "        engine_data.append(data[i-t:i,0:2])  # first two are unit_number, time_cycles\n",
        "        context_data.append(data[i-t:i,2:4]) # then settings data\n",
        "        input_data.append(data[i-t:i, 4:m-1])  #next attributes are sensor data\n",
        "        output_data.append([data[i+k-1:i+k,m-1]])  #last column is the RUL\n",
        "\n",
        "\n",
        "\n",
        "    U = np.array(engine_data)\n",
        "    X = np.array(input_data)\n",
        "    Y = np.array(output_data)\n",
        "    Z = np.array(context_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #print(n,m)\n",
        "#    print(X.shape)\n",
        "#     print(Y.shape)\n",
        "#     print(Z.shape)\n",
        "\n",
        "\n",
        "    return U, X, Y, Z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e287f36",
      "metadata": {
        "id": "7e287f36"
      },
      "outputs": [],
      "source": [
        "#for test set label is also provided\n",
        "\n",
        "def data_preparation_test(data,n_past,n_future):\n",
        "\n",
        "    '''\n",
        "    input:\n",
        "        data :[unit_number, time_cycles,context inputs, primary inputs, output, label]\n",
        "        n_past : number of past steps to be used for prediction\n",
        "        n_future :  number of steps ahead\n",
        "\n",
        "    returns:\n",
        "        context input (Z): 'setting_1','setting_2', 'setting_3'\n",
        "        primary input (X): 's_2','s_8','s_14','s_16'\n",
        "        ouput/target (Y): 'RUL' at time step t\n",
        "        Engine unit and time cycles (U)\n",
        "        label : label at time step t required for denormalisation of dat\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    n,m = data.shape\n",
        "    #print(n,m)\n",
        "\n",
        "    k = n_future\n",
        "    t = n_past\n",
        "\n",
        "\n",
        "    input_data = []\n",
        "    output_data = []\n",
        "    context_data = []\n",
        "    engine_data = []\n",
        "\n",
        "    label_data = []\n",
        "\n",
        "\n",
        "    for i in range(t, (n-k+1)):\n",
        "        engine_data.append(data[i-t:i,0:2])        # first two are unit_number, time_cycles\n",
        "        context_data.append(data[i-t:i,2:5])       # then settings data\n",
        "        input_data.append(data[i-t:i, 5:m-2])      # next 4 attributes are sensor data\n",
        "        output_data.append(data[i+k-1:i+k,m-2])    # second last column is the RUL\n",
        "        label_data.append(data[i+k-1:i+k,m-1])     #last column is label\n",
        "\n",
        "\n",
        "    U = np.array(engine_data)\n",
        "    X = np.array(input_data)\n",
        "    Y = np.array(output_data)\n",
        "    Z = np.array(context_data)\n",
        "    label = np.array(label_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return U, X, Y, Z, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac72c3d7",
      "metadata": {
        "id": "ac72c3d7"
      },
      "source": [
        "### b) Data preparation Training and validation (Minmax normalised data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf335b5",
      "metadata": {
        "id": "2cf335b5",
        "outputId": "1ff2b0eb-97cf-4b56-bf60-a5e21f688a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14206, 20, 13)\n",
            "(14206, 20, 2)\n",
            "(14206, 1, 1)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "prediction n_future = step ahead with n_past samples\n",
        "NOTE: n_future is set to 1 as the predicted step will not be k step ahead it will be at t+1\n",
        "unit_number is also present in columns, it will be used for selecting rows for validation set\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = smooth_data_train\n",
        "\n",
        "\n",
        "'''\n",
        "The train and test data is availble. Divide the given train data into two parts train and validation,\n",
        "considering the engine unit number.\n",
        "'''\n",
        "seq_len = 20 #sequence length (tune with 10,15,20)\n",
        "\n",
        "\n",
        "#take last rows equivalent to double the seq_len from each of the engine unit as validation data\n",
        "list_unit = train_FD001['unit_number'].unique()\n",
        "eng_data = data[data[:,0]==list_unit[0]]\n",
        "data_train = eng_data[:-2*seq_len,:]\n",
        "data_val = eng_data[-2*seq_len:,:]\n",
        "\n",
        "\n",
        "for i in range(1,len(list_unit)):\n",
        "    eng_data = data[data[:,0]==list_unit[i]]\n",
        "    data_train = np.concatenate((data_train,eng_data[:-2*seq_len,:]), axis=0)\n",
        "    data_val = np.concatenate((data_val,eng_data[-2*seq_len:,:]), axis=0)\n",
        "\n",
        "# print(data_train.shape)\n",
        "# print(data_val.shape)\n",
        "\n",
        "#prepare the data for each engine unit grp separately\n",
        "#X, Y, Z are 3 dim (num_samples,Sequence length, num_features)\n",
        "grp_data = data_train[data_train[:,0] == 1]\n",
        "U_train, X_train, Y_train, Z_train= data_preparation(grp_data,seq_len,1)  #last parameter 0 means context features excluded\n",
        "grp_data = data_val[data_val[:,0]==1]\n",
        "U_val, X_val, Y_val, Z_val = data_preparation(grp_data,seq_len,1)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_val.shape)\n",
        "\n",
        "\n",
        "list_unit = list_unit[1:]\n",
        "for i in range(1,len(list_unit)):\n",
        "    grp_data = data_train[data_train[:,0] == list_unit[i]]\n",
        "    U_train1, X_train1, Y_train1, Z_train1 = data_preparation(grp_data,seq_len,1)\n",
        "    grp_data = data_val[data_val[:,0]==i]\n",
        "    U_val1, X_val1, Y_val1, Z_val1 = data_preparation(grp_data,seq_len,1)\n",
        "\n",
        "\n",
        "    U_train = np.concatenate((U_train,U_train1),axis = 0)\n",
        "    X_train = np.concatenate((X_train,X_train1),axis = 0)\n",
        "    Y_train = np.concatenate((Y_train,Y_train1),axis = 0)\n",
        "    Z_train = np.concatenate((Z_train,Z_train1),axis = 0)\n",
        "\n",
        "\n",
        "    U_val = np.concatenate((U_val,U_val1),axis = 0)\n",
        "    X_val = np.concatenate((X_val,X_val1),axis = 0)\n",
        "    Y_val = np.concatenate((Y_val,Y_val1),axis = 0)\n",
        "    Z_val = np.concatenate((Z_val,Z_val1),axis = 0)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Z_train.shape)\n",
        "print(Y_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0528e02b",
      "metadata": {
        "id": "0528e02b"
      },
      "source": [
        "## 4. Loading data into PyTorch Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5510f9",
      "metadata": {
        "id": "aa5510f9",
        "outputId": "a21dd4a9-f207-404e-8588-8636d7150bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 20, 13]) torch.Size([64, 20, 2]) torch.Size([64, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "#transform the arrays into torch tensors\n",
        "train_features = torch.Tensor(X_train)  #drop unit_number and test_cycles\n",
        "train_targets = torch.Tensor(Y_train)\n",
        "train_cx_features = torch.Tensor(Z_train)\n",
        "\n",
        "val_features = torch.Tensor(X_val)\n",
        "val_targets = torch.Tensor(Y_val)\n",
        "val_cx_features = torch.Tensor(Z_val)\n",
        "\n",
        "\n",
        "train = TensorDataset(train_features,train_cx_features, train_targets)\n",
        "val = TensorDataset(val_features, val_cx_features,val_targets)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "examples = iter(train_loader)\n",
        "samples,context,targets = examples.next()\n",
        "print(samples.shape, context.shape,targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Hyperparameter tuning using Optuna"
      ],
      "metadata": {
        "id": "DmdlIMoDC5uL"
      },
      "id": "DmdlIMoDC5uL"
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter optimization with optuna\n",
        "\n",
        "input_dim = X_train.shape[2]\n",
        "output_dim = Y_train.shape[2]\n",
        "context_dim = Z_train.shape[2]\n",
        "\n",
        "weight_decay = 1e-6\n",
        "dropout = 0.1\n",
        "n_epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    params1 = {\n",
        "              'input_dim':input_dim,\n",
        "              'output_dim': output_dim,\n",
        "              'context_dim': context_dim,\n",
        "              'hidden_dim':trial.suggest_int('hidden_size',10,30,5),\n",
        "              }\n",
        "\n",
        "    params2 = {\n",
        "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
        "                'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
        "              }\n",
        "\n",
        "    #model = get_model('gru',params1)\n",
        "    model = ContextGRU(input_dim, params1['hidden_dim'], output_dim, context_dim)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"mean\")\n",
        "    #optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=weight_decay)\n",
        "    optimizer = getattr(optim, params2['optimizer'])(model.parameters(), lr= params2['learning_rate'], weight_decay=weight_decay)\n",
        "    opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
        "    train_loss = opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, np_features=input_dim, nc_features=context_dim)\n",
        "    opt.plot_losses()\n",
        "\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
        "study.optimize(objective, n_trials=15)\n",
        "\n",
        "\n",
        "best_trial = study.best_trial\n",
        "\n",
        "for key, value in best_trial.params.items():\n",
        "    print(\"{}: {}\".format(key, value))\n",
        "\n"
      ],
      "metadata": {
        "id": "S1AB1nSiE5LC"
      },
      "id": "S1AB1nSiE5LC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4f263054",
      "metadata": {
        "id": "4f263054"
      },
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "76c3ad92",
      "metadata": {
        "id": "76c3ad92"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "input_dim = X_train.shape[2]\n",
        "output_dim = Y_train.shape[2]\n",
        "context_dim = Z_train.shape[2]\n",
        "\n",
        "hidden_dim = 15  #tune the parameter 10,15,20\n",
        "layer_dim = 1\n",
        "batch_size = 64  #tune the parameter 64,128,256\n",
        "dropout = 0.2\n",
        "n_epochs = 100\n",
        "learning_rate = 0.001 #tune the parameter with optuna\n",
        "weight_decay = 1e-6\n",
        "\n",
        "model = ContextGRU(input_dim, hidden_dim, output_dim, context_dim)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "params_list = model.parameters()\n",
        "\n",
        "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
        "\n",
        "#optimizer = optim.Adam(params_list,lr=learning_rate, weight_decay=weight_decay)\n",
        "optimizer = optim.RMSprop(params_list, lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=weight_decay)\n",
        "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
        "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, np_features=input_dim, nc_features=context_dim)\n",
        "opt.plot_losses()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21fc8332",
      "metadata": {
        "id": "21fc8332"
      },
      "source": [
        "## 7. Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d01b0b",
      "metadata": {
        "id": "14d01b0b"
      },
      "source": [
        "### Testing (minmax normalisation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "22f06585",
      "metadata": {
        "id": "22f06585"
      },
      "outputs": [],
      "source": [
        "#Data prepartion for test data\n",
        "#colums: unit_number,time_cycles,setting_1,setting_2,setting_3,s_1,s_2,s_8,s_13,s_14,s_19,RUL,labels\n",
        "\n",
        "seq_len = 20  #sequence length for lstm\n",
        "\n",
        "\n",
        "data_test = smooth_data_test  #label is the last column, needed for denormalization\n",
        "\n",
        "\n",
        "list_unit = test_FD001['unit_number'].unique()\n",
        "\n",
        "\n",
        "#parameters from train data statistics\n",
        "par1 = p1[-1]  #parameter for 'RUL'  (mean/miu)\n",
        "par2 = p2[-1]  #parameter for 'RUL'  (range/sigma)\n",
        "\n",
        "#prepare the data for each engine unit grp separately\n",
        "#X, Y, Z are 3 dim (num_samples,Sequence length, num_features)\n",
        "#list_unit = [83]\n",
        "result_rmse = []\n",
        "result_r2 = []\n",
        "score = []\n",
        "fig_count = 0\n",
        "\n",
        "for i in list_unit:\n",
        "\n",
        "\n",
        "    grp_data = data_test[data_test[:,0] == i]\n",
        "    U_test, X_test, Y_test, Z_test = data_preparation(grp_data,seq_len,1)\n",
        "\n",
        "    if len(X_test) == 0:\n",
        "        continue\n",
        "\n",
        "    test_features = torch.Tensor(X_test)\n",
        "    test_targets = torch.Tensor(Y_test)\n",
        "    test_cx_features = torch.Tensor(Z_test)\n",
        "\n",
        "    test = TensorDataset(test_features,test_cx_features, test_targets)\n",
        "\n",
        "    #test_loader = DataLoader(test, batch_size=X_test.shape[0], shuffle=False, drop_last=True)\n",
        "    test_loader_one = DataLoader(test, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "    predictions, values = opt.evaluate(test_loader_one, batch_size=1, np_features=input_dim, nc_features = context_dim)\n",
        "    #flatten the multi-dimension array to 1-D array\n",
        "    vals = np.concatenate(values, axis=0).ravel()\n",
        "    preds = np.concatenate(predictions, axis=0).ravel()\n",
        "\n",
        "    #Apply inverse transform\n",
        "    #reshape vals and preds as inverse transform accepts 2-D array\n",
        "\n",
        "\n",
        "    #1 level denormalise the target\n",
        "    target_val = d_norm_1(np.reshape(vals,(len(vals),1)), \"minmax\",par1,par2)\n",
        "    pred_val = d_norm_1(np.reshape(preds,(len(preds),1)),\"minmax\",par1,par2)\n",
        "\n",
        "    # #plot_results(i, target_val,pred_val)\n",
        "    # if (fig_count % 15 == 0):\n",
        "    #     plot_results(i, target_val,pred_val)\n",
        "\n",
        "\n",
        "    result_metrics = calculate_metrics(target_val, pred_val)  #result_metrics is a dictionary\n",
        "    result_rmse.append(result_metrics['rmse'])\n",
        "    result_r2.append(result_metrics['r2'])\n",
        "    score.append(calculate_score(target_val, pred_val))\n",
        "\n",
        "#calculate average and std of rmse,and score\n",
        "\n",
        "avg_rmse = np.mean(result_rmse)\n",
        "std_rmse = np.std(result_rmse)\n",
        "\n",
        "avg_score = np.mean(score)\n",
        "std_score = np.std(score)\n",
        "\n",
        "print(f\"[Average RMSE: {avg_rmse:.4f}\\t Std RMSE: {std_rmse:.4f}]\")\n",
        "print(f\"[Average SCORE: {avg_score:.4f}\\t Std SCORE: {std_score:.4f}]\")\n",
        "\n",
        "min_rmse = min(result_rmse)\n",
        "max_rmse = max(result_rmse)\n",
        "min_score = min(score)\n",
        "max_score = max(score)\n",
        "print(f\"[Engine unit#: {result_rmse.index(min_rmse)+1}\\t Min RMSE: {min_rmse:.4f}]\")\n",
        "print(f\"[Engine unit#: {result_rmse.index(max_rmse)+1}\\t Max RMSE: {max_rmse:.4f}]\")\n",
        "print(f\"[Engine unit#: {score.index(min_score)+1}\\t Min score: {min_score:.4f}]\")\n",
        "print(f\"[Engine unit#: {score.index(max_score)+1}\\t Max score: {max_score:.4f}]\")\n",
        "\n",
        "print(torch.mode(torch.tensor(result_rmse),0))\n",
        "print(torch.mode(torch.tensor(score),0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41d497e",
      "metadata": {
        "id": "b41d497e",
        "outputId": "7238a1ce-f159-47de-de3e-f73763f9abbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[24.72247695315376]\n",
            "[1220663.5818933274]\n"
          ]
        }
      ],
      "source": [
        "# Test the whole data (not engine-wise) - minmax normalised\n",
        "#Data prepartion for test data\n",
        "\n",
        "seq_len = 20  #sequence length for lstm\n",
        "#cols = [0,1,2,3,4,5,6,13,18,19,24,26]   #0: unit number,1:time_cycles, 2,3,4 : 3 settings, 5,6,13,18,19: sensor data, 26:RUL\n",
        "#est_arr_data = test_norm_data.copy(deep = True)\n",
        "\n",
        "#remove labels for prediction task\n",
        "#test_arr_data.drop('label',axis=1, inplace = True)\n",
        "#data_test = test_arr_data.to_numpy()\n",
        "\n",
        "data_test = smooth_data_test\n",
        "\n",
        "#parameters required for inverse transform (cluster statistics)\n",
        "# cpar1 = param1\n",
        "# cpar2 = param2\n",
        "\n",
        "#parameters from train data statistics\n",
        "par1 = p1[-1]\n",
        "par2 = p2[-1]\n",
        "\n",
        "result_rmse = []\n",
        "result_r2 = []\n",
        "score = []\n",
        "\n",
        "\n",
        "#U_test, X_test, Y_test, Z_test, Y_labels = data_preparation_test(data_test,seq_len,1)\n",
        "U_test, X_test, Y_test, Z_test = data_preparation(data_test,seq_len,1)\n",
        "\n",
        "test_features = torch.Tensor(X_test)\n",
        "test_targets = torch.Tensor(Y_test)\n",
        "test_cx_features = torch.Tensor(Z_test)\n",
        "\n",
        "test = TensorDataset(test_features,test_cx_features, test_targets)\n",
        "\n",
        "#test_loader = DataLoader(test, batch_size=X_test.shape[0], shuffle=False, drop_last=True)\n",
        "test_loader_one = DataLoader(test, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "predictions, values = opt.evaluate(test_loader_one, batch_size=1, np_features=input_dim, nc_features = context_dim)\n",
        "#flatten the multi-dimension array to 1-D array\n",
        "vals = np.concatenate(values, axis=0).ravel()\n",
        "preds = np.concatenate(predictions, axis=0).ravel()\n",
        "\n",
        "\n",
        "#Apply denormalization transform\n",
        "#reshape vals and preds as inverse transform accepts 2-D array\n",
        "\n",
        "# target_val = d_norm(np.reshape(vals,(len(vals),1)), \"minmax\",Y_labels, cpar1, cpar2, par1,par2)\n",
        "# pred_val = d_norm(np.reshape(preds,(len(preds),1)),\"minmax\",Y_labels, cpar1, cpar2,par1,par2)\n",
        "\n",
        "\n",
        "target_val = d_norm_1(np.reshape(vals,(len(vals),1)), \"minmax\",par1,par2)\n",
        "pred_val = d_norm_1(np.reshape(preds,(len(preds),1)),\"minmax\",par1,par2)\n",
        "\n",
        "\n",
        "#plot_results(i, target_val,pred_val)\n",
        "\n",
        "#smooth the values\n",
        "#sm_pred_val = moving_average(pred_val, 10) #3 is window size\n",
        "\n",
        "result_metrics = calculate_metrics(target_val, pred_val)  #result_metrics is a dictionary\n",
        "result_rmse.append(result_metrics['rmse'])\n",
        "result_r2.append(result_metrics['r2'])\n",
        "score.append(calculate_score(target_val, pred_val))\n",
        "\n",
        "print(result_rmse)\n",
        "print(score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a5e21c8c",
      "metadata": {
        "id": "a5e21c8c"
      },
      "outputs": [],
      "source": [
        "# sns.set_palette(\"bright\")\n",
        "# sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "# plot_results(10, target_val[600:1000],pred_val[600:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c6e910",
      "metadata": {
        "id": "22c6e910"
      },
      "outputs": [],
      "source": [
        "#Plot the results Engine Unit wise\n",
        "def plot_results(unit_num, target_val,pred_val):\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    #plt.subplot(1,2,1).set_title(\"Engine Unit #\" + str(unit_num))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(target_val,'r')\n",
        "    plt.plot(pred_val,'b')\n",
        "    plt.xlabel('Time (Cycles)')\n",
        "    plt.ylabel('RUL')\n",
        "    plt.legend(['Actual RUL','Predicted RUL'])\n",
        "\n",
        "#     plt.subplot(1,2,2).set_title(\"Engine Unit #\" + str(unit_num))\n",
        "#     plt.plot(target_val,pred_val,'.r')\n",
        "#     plt.xlabel('Actual RUL')\n",
        "#     plt.ylabel('Predcited RUL')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f129080",
      "metadata": {
        "id": "5f129080"
      },
      "source": [
        "### Calculate Error Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f8b7eb",
      "metadata": {
        "id": "79f8b7eb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def calculate_metrics(actual, predicted):\n",
        "\n",
        "    return {'mae' : mean_absolute_error(actual,predicted),\n",
        "            'rmse' : mean_squared_error(actual,predicted) ** 0.5,\n",
        "            'r2' : r2_score(actual,predicted)}\n",
        "\n",
        "# result_metrics = calculate_metrics(target_val, pred_val)\n",
        "# print(result_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1976db26",
      "metadata": {
        "id": "1976db26"
      },
      "source": [
        "### Calculate Asymmetric score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772cc745",
      "metadata": {
        "id": "772cc745"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "#call this function for each unit, and sum to get the overall score\n",
        "def calculate_score(actual, predicted):\n",
        "#calcualate score for one engine unit\n",
        "    s = 0\n",
        "    a1 = 10 #constant\n",
        "    a2 = 13\n",
        "    s_list = []\n",
        "    d_list = predicted - actual\n",
        "    for i in range(len(actual)):\n",
        "        d = predicted[i]-actual[i]\n",
        "\n",
        "        if d >= 0:\n",
        "            #s = s+(math.exp(d/a2)-1)\n",
        "            s = (math.exp(d/a2)-1)\n",
        "            s_list.append(s)\n",
        "\n",
        "        else:\n",
        "\n",
        "            #s = s+(math.exp(-d/a1)-1)\n",
        "            s = (math.exp(-d/a1)-1)\n",
        "            s_list.append(s)\n",
        "\n",
        "#     plt.figure()\n",
        "#     plt.plot(d_list,s_list,'.')\n",
        "\n",
        "    return sum(s_list)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a19cf8",
      "metadata": {
        "id": "a2a19cf8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}